{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Hhnlrd5V32y"
      },
      "source": [
        "# Natural Language processing (dataset: 20 Newsgroups)\n",
        "- Stem word: `Speak` for speaking, spoken, speaks, etc. These words should be treated as same, by the ML Model.\n",
        "    - Stemming: Crop `ing` from speak, etc.\n",
        "    - Lemmatization: Find dictionary equivalent of the words.\n",
        "- `n_jobs = -1`: Use parallel processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YoyeKg91FDpT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7oaQiSvmI8c"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from sklearn.datasets import fetch_20newsgroups             # 20 categories of news\n",
        "import numpy as np\n",
        "from collections import defaultdict                         # Does not throw KeyError, unlike dict = {}\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import names                               # 5001 female and 2943 male names\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import timeit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEh-A_WJmUg3",
        "outputId": "a5822e5a-6e5b-4494-f91f-66295396103f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package names to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/names.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download('names')\n",
        "nltk.download('wordnet')                                         # Graph of words\n",
        "\n",
        "groups = fetch_20newsgroups()\n",
        "data_train = fetch_20newsgroups(subset='train', random_state=21) # This subset contains the training data.\n",
        "train_label = data_train.target                                  # Returns the label\n",
        "data_test = fetch_20newsgroups(subset='test', random_state=21)   # This subset contains the testing data.\n",
        "test_label = data_test.target\n",
        "len(data_train.data), len(data_test.data), len(test_label)\n",
        "np.unique(test_label)\n",
        "\n",
        "all_names = names.words()\n",
        "WNL = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QD-PhvOqkvu1",
        "outputId": "87b31c10-232c-44b5-c03f-a1e9d98f46d3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((11314, 1000), (7532, 1000))"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def clean(data):\n",
        "  cleaned = defaultdict(list)\n",
        "  count = 0\n",
        "  for group in data:\n",
        "     for words in group.split():\n",
        "        if words.isalpha() and words not in all_names:\n",
        "            cleaned[count].append(WNL.lemmatize(words.lower()))\n",
        "     cleaned[count] = ' '.join(cleaned[count])\n",
        "     count +=1\n",
        "  return(list(cleaned.values()))\n",
        "\n",
        "x_train = clean(data_train.data)\n",
        "x_test = clean(data_test.data)\n",
        "tf = TfidfVectorizer(stop_words='english', max_features=1000)\n",
        "X_train = tf.fit_transform(x_train)\n",
        "X_test = tf.transform(x_test)\n",
        "X_train.shape, X_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhGq-1D1mfwj"
      },
      "outputs": [],
      "source": [
        "svc_lib = SVC(kernel = 'linear')\n",
        "parameters = {'C' : (0.5,1.0,10,100)}\n",
        "grid_search1 = GridSearchCV(svc_lib, parameters, n_jobs = -1, cv = 3)\n",
        "\n",
        "start_time = timeit.default_timer()\n",
        "grid_search1.fit(X_train, train_label)\n",
        "final = timeit.default_timer()-start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_7JB3oMmPMK",
        "outputId": "75fbf555-c8e1-4187-aeed-d6b10a8115ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Execution Time :  240.55116530200007\n",
            "{'C': 1.0}\n",
            "0.7209652808886706\n",
            "0.6274561869357408\n"
          ]
        }
      ],
      "source": [
        "print(\"Execution Time : \",final)\n",
        "print(grid_search1.best_params_)\n",
        "print(grid_search1.best_score_)\n",
        "grid_search_best1 = grid_search1.best_estimator_\n",
        "accuracy = grid_search_best1.score(X_test, test_label)\n",
        "print(accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
