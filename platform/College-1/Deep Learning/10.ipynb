{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1t7OEhCG63fi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras import Model\n",
        "from keras.layers import Layer\n",
        "from keras.layers import Input, Dense, SimpleRNN\n",
        "import tensorflow as tf                                # Tensorflow META Package: tf\n",
        "import tensorflow.keras.activations as tfa             # Tensorflow -> Keras -> Activations = tfa\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.metrics import mean_squared_error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZgGVhZ816-bo",
        "outputId": "6689cded-a24b-4c75-8139-20dba710998e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 1.  2.  3.  5.  8. 13. 21. 34. 55. 89.]\n"
          ]
        }
      ],
      "source": [
        "# Prepare the dataset\n",
        "# Generate the Fibonacci Sequence\n",
        "def get_fib_seq(n, scale_data=True):\n",
        "    # Get the Fibonacci sequence\n",
        "    seq = np.zeros(n)\n",
        "    fib_n1 = 0.0\n",
        "    fib_n = 1.0\n",
        "    for i in range(n):\n",
        "        seq[i] = fib_n1 + fib_n\n",
        "        fib_n1 = fib_n\n",
        "        fib_n = seq[i]\n",
        "    scaler = []\n",
        "    if scale_data:\n",
        "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "        seq = np.reshape(seq, (n, 1))\n",
        "        seq = scaler.fit_transform(seq).flatten()\n",
        "    return seq, scaler\n",
        "\n",
        "fib_seq = get_fib_seq(10, False)[0]\n",
        "print(fib_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3S708qBf7UCx",
        "outputId": "a5f187d3-c19d-4be3-cda2-3152fd386b1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainX =  [[[ 8.]\n",
            "  [13.]\n",
            "  [21.]]\n",
            "\n",
            " [[ 5.]\n",
            "  [ 8.]\n",
            "  [13.]]\n",
            "\n",
            " [[ 2.]\n",
            "  [ 3.]\n",
            "  [ 5.]]\n",
            "\n",
            " [[13.]\n",
            "  [21.]\n",
            "  [34.]]\n",
            "\n",
            " [[21.]\n",
            "  [34.]\n",
            "  [55.]]\n",
            "\n",
            " [[34.]\n",
            "  [55.]\n",
            "  [89.]]]\n",
            "trainY =  [ 34.  21.   8.  55.  89. 144.]\n"
          ]
        }
      ],
      "source": [
        "# Get Compile Training Examples and Target values\n",
        "def get_fib_XY(total_fib_numbers, time_steps, train_percent, scale_data=True):\n",
        "    dat, scaler = get_fib_seq(total_fib_numbers, scale_data)\n",
        "    Y_ind = np.arange(time_steps, len(dat), 1)\n",
        "    Y = dat[Y_ind]\n",
        "    rows_x = len(Y)\n",
        "    X = dat[0:rows_x]\n",
        "    for i in range(time_steps-1):\n",
        "        temp = dat[i+1:rows_x+i+1]\n",
        "        X = np.column_stack((X, temp))\n",
        "    # random permutation with fixed seed\n",
        "    rand = np.random.RandomState(seed=13)\n",
        "    idx = rand.permutation(rows_x)\n",
        "    split = int(train_percent*rows_x)\n",
        "    train_ind = idx[0:split]\n",
        "    test_ind = idx[split:]\n",
        "    trainX = X[train_ind]\n",
        "    trainY = Y[train_ind]\n",
        "    testX = X[test_ind]\n",
        "    testY = Y[test_ind]\n",
        "    trainX = np.reshape(trainX, (len(trainX), time_steps, 1))\n",
        "    testX = np.reshape(testX, (len(testX), time_steps, 1))\n",
        "    return trainX, trainY, testX, testY, scaler\n",
        "\n",
        "trainX, trainY, testX, testY, scaler = get_fib_XY(12, 3, 0.7, False)\n",
        "print('trainX = ', trainX)\n",
        "print('trainY = ', trainY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsJSPQYc7p5W",
        "outputId": "3ac5e5b4-33d2-430e-9f7c-92603b7ae702"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "826/826 - 4s - 5ms/step - loss: 6.2155e-04\n",
            "Epoch 2/30\n",
            "826/826 - 4s - 5ms/step - loss: 4.7133e-04\n",
            "Epoch 3/30\n",
            "826/826 - 2s - 3ms/step - loss: 3.6114e-04\n",
            "Epoch 4/30\n",
            "826/826 - 3s - 3ms/step - loss: 2.6951e-04\n",
            "Epoch 5/30\n",
            "826/826 - 3s - 3ms/step - loss: 2.1609e-04\n",
            "Epoch 6/30\n",
            "826/826 - 5s - 6ms/step - loss: 1.4797e-04\n",
            "Epoch 7/30\n",
            "826/826 - 2s - 3ms/step - loss: 1.1758e-04\n",
            "Epoch 8/30\n",
            "826/826 - 2s - 3ms/step - loss: 8.9288e-05\n",
            "Epoch 9/30\n",
            "826/826 - 3s - 4ms/step - loss: 7.7293e-05\n",
            "Epoch 10/30\n",
            "826/826 - 5s - 6ms/step - loss: 7.0602e-05\n",
            "Epoch 11/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.8970e-05\n",
            "Epoch 12/30\n",
            "826/826 - 2s - 2ms/step - loss: 7.1050e-05\n",
            "Epoch 13/30\n",
            "826/826 - 3s - 4ms/step - loss: 6.8680e-05\n",
            "Epoch 14/30\n",
            "826/826 - 5s - 6ms/step - loss: 6.7824e-05\n",
            "Epoch 15/30\n",
            "826/826 - 2s - 2ms/step - loss: 6.4311e-05\n",
            "Epoch 16/30\n",
            "826/826 - 3s - 3ms/step - loss: 6.7137e-05\n",
            "Epoch 17/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.2809e-05\n",
            "Epoch 18/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.4777e-05\n",
            "Epoch 19/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.1824e-05\n",
            "Epoch 20/30\n",
            "826/826 - 3s - 3ms/step - loss: 6.3943e-05\n",
            "Epoch 21/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.2396e-05\n",
            "Epoch 22/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.3831e-05\n",
            "Epoch 23/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.0818e-05\n",
            "Epoch 24/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.0346e-05\n",
            "Epoch 25/30\n",
            "826/826 - 3s - 3ms/step - loss: 6.3244e-05\n",
            "Epoch 26/30\n",
            "826/826 - 3s - 3ms/step - loss: 5.9142e-05\n",
            "Epoch 27/30\n",
            "826/826 - 3s - 4ms/step - loss: 5.8130e-05\n",
            "Epoch 28/30\n",
            "826/826 - 4s - 5ms/step - loss: 5.8527e-05\n",
            "Epoch 29/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.1084e-05\n",
            "Epoch 30/30\n",
            "826/826 - 3s - 3ms/step - loss: 5.7190e-05\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - loss: 4.5949e-05\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 3.2871e-05\n",
            "Train set MSE =  4.1742379835341126e-05\n",
            "Test set MSE =  1.6495676391059533e-05\n"
          ]
        }
      ],
      "source": [
        "# Predefined parameters\n",
        "time_steps = 20\n",
        "hidden_units = 2\n",
        "epochs = 30\n",
        "\n",
        "# Function to create Simple RNN\n",
        "def create_RNN(hidden_units, dense_units, input_shape, activation):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(hidden_units, input_shape=input_shape, activation=activation[0]))\n",
        "    model.add(Dense(units=dense_units, activation=activation[1]))\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# 1. Generate dataset\n",
        "trainX, trainY, testX, testY, scaler = get_fib_XY(1200, time_steps, 0.7)\n",
        "\n",
        "# 2. Model Created: model_RNN\n",
        "model_RNN = create_RNN(hidden_units=hidden_units, dense_units=1, input_shape=(time_steps,1),\n",
        "activation=['tanh', 'tanh'])\n",
        "\n",
        "# 2.1 Visualize\n",
        "# model_RNN.summary()\n",
        "\n",
        "# 3. Train the network\n",
        "model_RNN.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
        "\n",
        "# 4. Evaluate model\n",
        "train_mse = model_RNN.evaluate(trainX, trainY)\n",
        "test_mse = model_RNN.evaluate(testX, testY)\n",
        "\n",
        "# 5. Print error\n",
        "print(\"Train set MSE = \", train_mse)\n",
        "print(\"Test set MSE = \", test_mse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMIZ2lHO8fRp"
      },
      "outputs": [],
      "source": [
        "# Function to Build the Attention layer\n",
        "class attention(Layer):\n",
        "    def __init__(self,**kwargs):\n",
        "        super(attention,self).__init__(**kwargs)\n",
        "\n",
        "    def build(self,input_shape):\n",
        "        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),\n",
        "        initializer='random_normal', trainable=True)\n",
        "        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),\n",
        "        initializer='zeros', trainable=True)\n",
        "        super(attention, self).build(input_shape)\n",
        "\n",
        "    def call(self,x):\n",
        "        # Alignment scores. Pass them through tanh function\n",
        "        e = tfa.tanh(tf.matmul(x,self.W)+self.b)\n",
        "        # Remove dimension of size 1\n",
        "        e = tf.squeeze(e, axis=-1)\n",
        "        # Compute the weights\n",
        "        alpha = tfa.softmax(e)\n",
        "        # Reshape to tensorFlow format\n",
        "        alpha = tf.expand_dims(alpha, axis=-1)\n",
        "        # Compute the context vector\n",
        "        context = x * alpha\n",
        "        context = tf.reduce_sum(context, axis=1)\n",
        "        return context\n",
        "\n",
        "# Function to Create RNN with the attention layer\n",
        "def create_RNN_with_attention(hidden_units, dense_units, input_shape, activation):\n",
        "    x=Input(shape=input_shape)\n",
        "    RNN_layer = SimpleRNN(hidden_units, return_sequences=True, activation=activation)(x)\n",
        "    attention_layer = attention()(RNN_layer)\n",
        "    outputs=Dense(dense_units, trainable=True, activation=activation)(attention_layer)\n",
        "    model=Model(x,outputs)\n",
        "    model.compile(loss='mse', optimizer='adam')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1g6acb6r8vSa",
        "outputId": "0c2878e7-b086-4923-d833-0ac60d2cba55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "826/826 - 5s - 6ms/step - loss: 0.0014\n",
            "Epoch 2/30\n",
            "826/826 - 2s - 3ms/step - loss: 0.0013\n",
            "Epoch 3/30\n",
            "826/826 - 2s - 3ms/step - loss: 0.0012\n",
            "Epoch 4/30\n",
            "826/826 - 3s - 3ms/step - loss: 0.0012\n",
            "Epoch 5/30\n",
            "826/826 - 2s - 3ms/step - loss: 0.0011\n",
            "Epoch 6/30\n",
            "826/826 - 2s - 3ms/step - loss: 0.0011\n",
            "Epoch 7/30\n",
            "826/826 - 2s - 3ms/step - loss: 0.0010\n",
            "Epoch 8/30\n",
            "826/826 - 2s - 3ms/step - loss: 9.6702e-04\n",
            "Epoch 9/30\n",
            "826/826 - 3s - 3ms/step - loss: 9.3256e-04\n",
            "Epoch 10/30\n",
            "826/826 - 3s - 3ms/step - loss: 8.7178e-04\n",
            "Epoch 11/30\n",
            "826/826 - 2s - 3ms/step - loss: 8.1538e-04\n",
            "Epoch 12/30\n",
            "826/826 - 2s - 3ms/step - loss: 7.6302e-04\n",
            "Epoch 13/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.8534e-04\n",
            "Epoch 14/30\n",
            "826/826 - 2s - 3ms/step - loss: 6.1483e-04\n",
            "Epoch 15/30\n",
            "826/826 - 3s - 3ms/step - loss: 5.4419e-04\n",
            "Epoch 16/30\n",
            "826/826 - 2s - 3ms/step - loss: 4.7591e-04\n",
            "Epoch 17/30\n",
            "826/826 - 2s - 3ms/step - loss: 4.0793e-04\n",
            "Epoch 18/30\n",
            "826/826 - 3s - 3ms/step - loss: 3.4287e-04\n",
            "Epoch 19/30\n",
            "826/826 - 2s - 3ms/step - loss: 2.8221e-04\n",
            "Epoch 20/30\n",
            "826/826 - 3s - 4ms/step - loss: 2.4256e-04\n",
            "Epoch 21/30\n",
            "826/826 - 5s - 6ms/step - loss: 2.0145e-04\n",
            "Epoch 22/30\n",
            "826/826 - 2s - 3ms/step - loss: 1.6168e-04\n",
            "Epoch 23/30\n",
            "826/826 - 2s - 3ms/step - loss: 1.3326e-04\n",
            "Epoch 24/30\n",
            "826/826 - 2s - 3ms/step - loss: 1.1793e-04\n",
            "Epoch 25/30\n",
            "826/826 - 2s - 3ms/step - loss: 1.0633e-04\n",
            "Epoch 26/30\n",
            "826/826 - 3s - 3ms/step - loss: 9.7934e-05\n",
            "Epoch 27/30\n",
            "826/826 - 2s - 3ms/step - loss: 9.2653e-05\n",
            "Epoch 28/30\n",
            "826/826 - 3s - 3ms/step - loss: 9.1593e-05\n",
            "Epoch 29/30\n",
            "826/826 - 3s - 3ms/step - loss: 9.1212e-05\n",
            "Epoch 30/30\n",
            "826/826 - 2s - 3ms/step - loss: 8.9524e-05\n",
            "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 6.2753e-05\n",
            "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 2.0624e-05\n",
            "Train set MSE with attention =  6.715073686791584e-05\n",
            "Test set MSE with attention =  8.268838428193703e-06\n"
          ]
        }
      ],
      "source": [
        "# 1. Dataset has already been generated\n",
        "\n",
        "# 2. Model Created: RNN with Attention Layer\n",
        "model_attention = create_RNN_with_attention(hidden_units=hidden_units, dense_units=1,input_shape=(time_steps,1), activation='tanh')\n",
        "\n",
        "# 2.1 Visualize:\n",
        "# model_attention.summary()\n",
        "\n",
        "# 3. Train\n",
        "model_attention.fit(trainX, trainY, epochs=epochs, batch_size=1, verbose=2)\n",
        "\n",
        "# 4. Evaluate\n",
        "train_mse_attn = model_attention.evaluate(trainX, trainY)\n",
        "test_mse_attn = model_attention.evaluate(testX, testY)\n",
        "\n",
        "# 5. Print error\n",
        "print(\"Train set MSE with attention = \", train_mse_attn)\n",
        "print(\"Test set MSE with attention = \", test_mse_attn)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
